{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a43845",
   "metadata": {},
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde0aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = Path(PATH+'/data/Test1')\n",
    "PATH_Result = Path(PATH+'/data/Results')\n",
    "\n",
    "PATH_Data.mkdir(parents=True, exist_ok=True)\n",
    "PATH_Result.mkdir(parents=True, exist_ok=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7bea157",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('P11')\n",
    "             .master('local')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14aeccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97bf13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PC-Nicyuz:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P11</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f38014b8c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemin : d:/Formations/Github/Projet_11/data/Test1\n"
     ]
    }
   ],
   "source": [
    "# chemin vers le rÃ©pertoire de test\n",
    "path = PATH_Data.as_posix()\n",
    "print(\"chemin : \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/d:/Formatio...|2021-09-12 20:23:56| 17398|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# une seule image ==> fonctionnelle\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "   .load(path + \"/r0_0.jpg\")\n",
    "images.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d4e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinaryFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpathGlobFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecursiveFileLookup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m images\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32md:\\Formations\\Github\\Projet_11\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32md:\\Formations\\Github\\Projet_11\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Formations\\Github\\Projet_11\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Formations\\Github\\Projet_11\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# forme indiquÃ© dans la doc, mais pas fonctionnelle pour une raison que je ne comprends pas\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "   .load(path)\n",
    "images.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683efd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forme fonctionnelle mais produit une boucle infinie\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "   .load(path + \"/*\")\n",
    "images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c024d",
   "metadata": {},
   "source": [
    "# CE QUI RESTE AU DESSOUS EST A IGNORER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faeaf",
   "metadata": {},
   "source": [
    "<u>Affichage des 5 premiÃ¨res images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa derniÃ¨re modification\n",
    " - sa longueur\n",
    " - son contenu encodÃ© en valeur hexadÃ©cimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863981e5",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d47705",
   "metadata": {},
   "source": [
    "### 3.7.2 PrÃ©paration du modÃ¨le\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\n",
    "J'ai choisi d'utiliser le modÃ¨le **MobileNetV2** pour sa rapiditÃ© d'exÃ©cution comparÃ©e <br />\n",
    "Ã  d'autres modÃ¨les comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\n",
    "je vous invite Ã  lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schÃ©ma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une derniÃ¨re couche qui sert Ã  classer les images <br />\n",
    "selon 1000 catÃ©gories que nous ne voulons pas utiliser.<br />\n",
    "L'idÃ©e dans ce projet est de rÃ©cupÃ©rer le **vecteur de caractÃ©ristiques <br />\n",
    "de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur <br />\n",
    "de classification Ã  reconnaitre les diffÃ©rents fruits du jeu de donnÃ©es.\n",
    "\n",
    "Comme d'autres modÃ¨les similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\n",
    "en incluant toutes ses couches, attend obligatoirement des images <br />\n",
    "de dimension (224,224,3). Nos images Ã©tant toutes de dimension (100,100,3), <br />\n",
    "nous devrons simplement les **redimensionner** avant de les confier au modÃ¨le.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modÃ¨le **MobileNetV2** avec les poids **prÃ©calculÃ©s** <br />\n",
    "    issus d'**imagenet** et en spÃ©cifiant le format de nos images en entrÃ©e\n",
    " 2. Nous crÃ©ons un nouveau modÃ¨le avec:\n",
    "  - <u>en entrÃ©e</u> : l'entrÃ©e du modÃ¨le MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant derniÃ¨re couche du modÃ¨le MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b197379",
   "metadata": {},
   "source": [
    "Affichage du rÃ©sumÃ© de notre nouveau modÃ¨le oÃ¹ nous constatons <br />\n",
    "que <u>nous rÃ©cupÃ©rons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8207725",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0adcf5",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir accÃ©der au modÃ¨le ainsi qu'Ã  ses poids. <br />\n",
    "Une bonne pratique consiste Ã  charger le modÃ¨le sur le driver puis Ã  diffuser <br />\n",
    "ensuite les poids aux diffÃ©rents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc53ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e34e",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd51ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620876",
   "metadata": {},
   "source": [
    "### 3.7.3 DÃ©finition du processus de chargement des images et application <br/>de leur featurisation Ã  travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook dÃ©finit la logique par Ã©tapes, jusqu'Ã  Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - featuriser une sÃ©rie d'images pd.Series\n",
    "   - prÃ©traiter une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf2ef9",
   "metadata": {},
   "source": [
    "### 3.7.4 ExÃ©cution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de trÃ¨s grandes images), <br />\n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
    "essayez de rÃ©duire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet <br />\n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8f95d",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant exÃ©cuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dÃ©pend du volume de donnÃ©es Ã  traiter. <br />\n",
    "\n",
    "Notre jeu de donnÃ©es de **Test** contient **22819 images**. <br />\n",
    "Cependant, dans l'exÃ©cution en mode **local**, <br />\n",
    "nous <u>traiterons un ensemble rÃ©duit de **330 images**</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e83ec",
   "metadata": {},
   "source": [
    "<u>Rappel du PATH oÃ¹ seront inscrits les fichiers au format \"**parquet**\" <br />\n",
    "contenant nos rÃ©sultats, Ã  savoir, un DataFrame contenant 3 colonnes</u> :\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caractÃ©ristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8901db3",
   "metadata": {},
   "source": [
    "<u>Enregistrement des donnÃ©es traitÃ©es au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9506f21",
   "metadata": {},
   "source": [
    "## 3.8 Chargement des donnÃ©es enregistrÃ©es et validation du rÃ©sultat\n",
    "\n",
    "<u>On charge les donnÃ©es fraichement enregistrÃ©es dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19243bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15070",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premiÃ¨res lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2794fca",
   "metadata": {},
   "source": [
    "<u>On valide que la dimension du vecteur de caractÃ©ristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb933b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5348d",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de donnÃ©es allÃ©gÃ© en local <br />\n",
    "oÃ¹ nous avons simulÃ© un cluster de machines en rÃ©partissant la charge de travail <br />\n",
    "sur diffÃ©rents cÅurs de processeur au sein d'une mÃªme machine.\n",
    "\n",
    "Nous allons maintenant gÃ©nÃ©raliser le processus en dÃ©ployant notre solution <br />\n",
    "sur un rÃ©el cluster de machines et nous travaillerons dÃ©sormais sur la totalitÃ© <br />\n",
    "des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f7c1",
   "metadata": {},
   "source": [
    "# 4. DÃ©ploiement de la solution sur le cloud\n",
    "\n",
    "Maintenant que nous avons vÃ©rifiÃ© que notre solution fonctionne, <br />\n",
    "il est temps de la <u>dÃ©ployer Ã  plus grande Ã©chelle sur un vrai cluster de machines</u>.\n",
    "\n",
    "**Attention**, *je travaille sous Linux avec une version Ubuntu, <br />\n",
    "les commandes dÃ©crites ci-dessous sont donc rÃ©alisÃ©es <br />\n",
    "exclusivement dans cet environnement.*\n",
    "\n",
    "<u>Plusieurs contraintes se posent</u> :\n",
    " 1. Quel prestataire de Cloud choisir ?\n",
    " 2. Quelles solutions de ce prestataire adopter ?\n",
    " 3. OÃ¹ stocker nos donnÃ©es ?\n",
    " 4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    " \n",
    "## 4.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le prestataire le plus connu et qui offre Ã  ce jour l'offre <br />\n",
    "la plus large dans le cloud computing est **Amazon Web Services** (AWS).<br />\n",
    "Certaines de leurs offres sont parfaitement adaptÃ©es Ã  notre problÃ©matique <br />\n",
    "et c'est la raison pour laquelle j'utiliserai leurs services.\n",
    "\n",
    "L'objectif premier est de pouvoir, grÃ¢ce Ã  AWS, <u>louer de la puissance de calcul Ã  la demande</u>. <br />\n",
    "L'idÃ©e Ã©tant de pouvoir, quel que soit la charge de travail, <br />\n",
    "obtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, <br />\n",
    "mÃªme si le volume de donnÃ©es venait Ã  fortement augmenter.\n",
    "\n",
    "De plus, la capacitÃ© d'utiliser cette puissance de calcul Ã  la demande <br />\n",
    "permet de diminuer drastiquement les coÃ»ts si l'on compare les coÃ»ts d'une location <br />\n",
    "de serveur complet sur une durÃ©e fixe (1 mois, 1 annÃ©e par exemple).\n",
    "\n",
    "## 4.2 Choix de la solution technique : EMR\n",
    "\n",
    "<u>Plusieurs solutions s'offre Ã  nous</u> :\n",
    "1. Solution **IAAS** (Infrastructure AS A Service)\n",
    " - Dans cette configuration **AWS** met Ã  notre disposition des serveurs vierges <br />\n",
    "   sur lequel nous avons un accÃ¨s en administrateur, ils sont nommÃ©s **instance EC2**.<br />\n",
    "   Pour faire simple, nous pouvons avec cette solution reproduire pratiquement <br />\n",
    "   Ã  l'identique la solution mis en Åuvre en local sur notre machine.<br />\n",
    "   <u>On installe nous-mÃªme l'intÃ©gralitÃ© des outils puis on exÃ©cute notre script</u> :\n",
    "  - Installation de **Spark**, **Java** etc.\n",
    "  - Installation de **Python** (via Anaconda par exemple)\n",
    "  - Installation de **Jupyter Notebook**\n",
    "  - Installation des **librairies complÃ©mentaires**\n",
    "  - Il faudra bien Ã©videment veiller Ã  **implÃ©menter les librairies \n",
    "    nÃ©cessaires Ã  toutes les machines (workers) du cluster**\n",
    "  - <u>Avantages</u> :\n",
    "      - LibertÃ© totale de mise en Åuvre de la solution\n",
    "      - FacilitÃ© de mise en Åuvre Ã  partir d'un modÃ¨le qui s'exÃ©cute en local sur une machine Linux\n",
    "  - <u>InconvÃ©nients</u> :\n",
    "      - Cronophage\n",
    "          - NÃ©cessitÃ© d'installer et de configurer toute la solution\n",
    "      - Possible problÃ¨mes techniques Ã  l'installation des outils (des problÃ©matiques qui <br />\n",
    "        n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n",
    "      - Solution non pÃ©renne dans le temps, il faudra veiller Ã  la mise Ã  jour des outils <br />\n",
    "        et Ã©ventuellement devoir rÃ©installer Spark, Java etc. \n",
    "2. Solution **PAAS** (Plateforme As A Service)\n",
    " - **AWS** fournit Ã©normÃ©ment de services diffÃ©rents, dans l'un de ceux-lÃ  <br />\n",
    "   il existe une offre qui permet de louer des **instances EC2** <br />\n",
    "   avec des applications prÃ©installÃ©es et configurÃ©es : il s'agit du **service EMR**.\n",
    " - **Spark** y sera dÃ©jÃ  installÃ©\n",
    " - PossibilitÃ© de demander l'installation de **Tensorflow** ainsi que **JupyterHub**\n",
    " - PossibilitÃ© d'indiquer des **packages complÃ©mentaires** Ã  installer <br />\n",
    "   Ã  l'initialisation du serveur **sur l'ensemble des machines du cluster**.\n",
    " - <u>Avantages</u> :\n",
    "     - FacilitÃ© de mise en Åuvre\n",
    "         - Il suffit de trÃ¨s peu de configuration pour obtenir <br />\n",
    "           un environnement parfaitement fonctionnel\n",
    "     - RapiditÃ© de mise en Åuvre\n",
    "         - Une fois la premiÃ¨re configuration rÃ©alisÃ©e, il est trÃ¨s facile <br />\n",
    "           et trÃ¨s rapide de recrÃ©er des clusters Ã  l'identique qui seront <br />\n",
    "           disponibles presque instantanÃ©ment (le temps d'instancier les <br />\n",
    "           serveurs soit environ 15/20 minutes)\n",
    "     - Solutions matÃ©rielless et logicielles optimisÃ©es par les ingÃ©nieurs d'AWS\n",
    "         - On sait que les versions installÃ©es vont fonctionner <br />\n",
    "           et que l'architecture proposÃ©e est optimisÃ©e\n",
    "     - StabilitÃ© de la solution\n",
    "    - Solution Ã©volutive\n",
    "        Il est facile dâobtenir Ã  chaque nouvelle instanciation une version Ã  jour <br />\n",
    "        de chaque package, en Ã©tant garanti de leur compatibilitÃ© avec le reste de lâenvironnement.\n",
    "  - Plus sÃ©curisÃ©\n",
    "\t- Les Ã©ventuels patchs de sÃ©curitÃ© seront automatiquement mis Ã  jour <br />\n",
    "      Ã  chaque nouvelle instanciation du cluster EMR.\n",
    " - <u>InconvÃ©nients</u> :\n",
    "     - Peut-Ãªtre un certain manque de libertÃ© sur la version des packages disponibles ? <br />\n",
    "       MÃªme si je n'ai pas constatÃ© ce problÃ¨me.\n",
    "   \n",
    "\n",
    "Je retiens la solution **PAAS** en choisissant d'utiliser <br />\n",
    "le service **EMR** d'Amazon Web Services.<br />\n",
    "Je la trouve plus adaptÃ©e Ã  notre problÃ©matique et permet <br />\n",
    "une mise en Åuvre qui soit Ã  la fois plus rapide et <br />\n",
    "plus efficace que la solution IAAS.\n",
    "\n",
    "## 4.3 Choix de la solution de stockage des donnÃ©es : Amazon S3\n",
    "\n",
    "<u>Amazon propose une solution trÃ¨s efficace pour la gestion du stockage des donnÃ©es</u> : **Amazon S3**. <br />\n",
    "S3 pour Amazon Simple Storage Service.\n",
    "\n",
    "Il pourrait Ãªtre tentant de stocker nos donnÃ©es sur l'espace allouÃ© par le serveur **EC2**, <br />\n",
    "mais si nous ne prenons aucune mesure pour les sauvegarder ensuite sur un autre support, <br />\n",
    "<u>les donnÃ©es seront perdues</u> lorsque le serveur sera rÃ©siliÃ© (on rÃ©silie le serveur lorsqu'on <br />\n",
    "ne s'en sert pas pour des raisons de coÃ»t).<br />\n",
    "De fait, si l'on dÃ©cide d'utiliser l'espace disque du serveur EC2 il faudra imaginer <br />\n",
    "une solution pour sauvegarder les donnÃ©es avant la rÃ©siliation du serveur.\n",
    "De plus, nous serions exposÃ©s Ã  certaines problÃ©matiques si nos donnÃ©es venaient Ã  <br />\n",
    "**saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n",
    "\n",
    "<u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces problÃ©matiques</u>. <br />\n",
    "L'espace disque disponible est **illimitÃ©**, et il est **indÃ©pendant de nos serveurs EC2**. <br />\n",
    "L'accÃ¨s aux donnÃ©es est **trÃ¨s rapide** car nous restons dans l'environnement d'AWS <br />\n",
    "et nous prenons soin de <u>choisir la mÃªme rÃ©gion pour nos serveurs **EC2** et **S3**</u>.\n",
    "\n",
    "De plus, comme nous le verrons <u>il est possible d'accÃ©der aux donnÃ©es sur **S3** <br />\n",
    "    de la mÃªme maniÃ¨re que l'on **accÃ¨de aux donnÃ©es sur un disque local**</u>.<br />\n",
    "Nous utiliserons simplement un **PATH au format s3://...** .\n",
    "\n",
    "## 4.4 Configuration de l'environnement de travail\n",
    "\n",
    "La premiÃ¨re Ã©tape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/),<br />\n",
    "il s'agit de l'**interface en ligne de commande d'AWS**.<br />\n",
    "Elle nous permet d'**interagir avec les diffÃ©rents services d'AWS**, comme **S3** par exemple.\n",
    "\n",
    "Pour pouvoir utiliser **AWS Cli**, il faut le configurer en crÃ©ant prÃ©alablement <br />\n",
    "un utilisateur Ã  qui on donnera les autorisations dont nous aurons besoin.<br />\n",
    "Dans ce projet il faut que l'utilisateur ait Ã  minima un contrÃ´le total sur le service S3.\n",
    "\n",
    "<u>La gestion des utilisateurs et de leurs droits s'effectue via le service **AMI**</u> d'AWS.\n",
    "\n",
    "Une fois l'utilisateur crÃ©Ã© et ses autorisations configurÃ©es nous crÃ©ons une **paire de clÃ©s** <br />\n",
    "qui nous permettra de nous **connecter sans Ã  avoir Ã  devoir saisir systÃ©matiquement notre login/mot de passe**.<br />\n",
    "\n",
    "Il faut Ã©galement configurer l'**accÃ¨s SSH** Ã  nos futurs serveurs EC2. <br />\n",
    "Ici aussi, via un systÃ¨me de clÃ©s qui nous dispense de devoir nous authentifier \"Ã  la main\" Ã  chaque connexion.\n",
    "\n",
    "Toutes ses Ã©tapes de configuration sont parfaitement dÃ©crites <br />\n",
    "dans le cours du projet: [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / DÃ©couvrez Amazon Web Services](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308686-decouvrez-amazon-web-services#/id/r-4355822)\n",
    "\n",
    "## 4.5 Upload de nos donnÃ©es sur S3\n",
    "\n",
    "Nos outils sont configurÃ©s. <br />\n",
    "Il faut maintenant uploader nos donnÃ©es de travail sur Amazon S3.\n",
    "\n",
    "Ici aussi les Ã©tapes sont dÃ©crites avec prÃ©cision <br />\n",
    "dans le cours [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / Stockez des donnÃ©es sur S3](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308691-stockez-des-donnees-sur-s3)\n",
    "\n",
    "Je dÃ©cide de n'uploader que les donnÃ©es contenues dans le dossier **Test** du [jeu de donnÃ©es du projet](https://www.kaggle.com/moltean/fruits/download)\n",
    "\n",
    "\n",
    "La premiÃ¨re Ã©tape consiste Ã  **crÃ©er un bucket sur S3** <br />\n",
    "dans lequel nous uploaderons les donnÃ©es du projet:\n",
    "- **aws s3 mb s3://p8-data**\n",
    "\n",
    "On vÃ©rifie que le bucket Ã  bien Ã©tÃ© crÃ©Ã©\n",
    "- **aws s3 ls**\n",
    " - Si le nom du bucket s'affiche alors c'est qu'il a Ã©tÃ© correctement crÃ©Ã©.\n",
    "\n",
    "On copie ensuite le contenu du dossier \"**Test**\" <br />\n",
    "dans un rÃ©pertoire \"**Test**\" sur notre bucket \"**p8-data**\":\n",
    "1. On se place Ã  l'intÃ©rieur du rÃ©pertoire **Test**\n",
    "2. **aws sync . s3://p8-data/Test**\n",
    "\n",
    "La commande **sync** est utile pour synchroniser deux rÃ©pertoires.\n",
    "\n",
    "<u>Nos donnÃ©es du projet sont maintenant disponibles sur Amazon S3</u>.\n",
    "\n",
    "## 4.6 Configuration du serveur EMR\n",
    "\n",
    "Une fois encore, le cours [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / DÃ©ployez un cluster de calculs distribuÃ©s](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> dÃ©taille l'essentiel des Ã©tapes pour lancer un cluster avec **EMR**.\n",
    "\n",
    "<u>Je dÃ©taillerai ici les Ã©tapes particuliÃ¨res qui nous permettent <br />\n",
    "de configurer le serveur selon nos besoins</u> :\n",
    "\n",
    "1. Cliquez sur CrÃ©er un cluster\n",
    "![CrÃ©er un cluster](img/EMR_creer.png)\n",
    "2. Cliquez sur AccÃ©der aux options avancÃ©es\n",
    "![CrÃ©er un cluster](img/EMR_options_avancees.png)\n",
    "\n",
    "### 4.6.1 Ãtape 1 : Logiciels et Ã©tapes\n",
    "\n",
    "#### 4.6.1.1 Configuration des logiciels\n",
    "\n",
    "<u>SÃ©lectionnez les packages dont nous aurons besoin comme dans la capture d'Ã©cran</u> :\n",
    "1. Nous sÃ©lectionnons la derniÃ¨re version d'**EMR**, soit la version **6.3.0** au moment oÃ¹ je rÃ©dige ce document\n",
    "2. Nous cochons bien Ã©videment **Hadoop** et **Spark** qui seront prÃ©installÃ©s dans leur version la plus rÃ©cente\n",
    "3. Nous aurons Ã©galement besoin de **TensorFlow** pour importer notre modÃ¨le et rÃ©aliser le **transfert learning**\n",
    "4. Nous travaillerons enfin avec un **notebook Jupyter** via l'application **JupyterHub**<br />\n",
    " - Comme nous le verrons dans un instant nous allons <u>paramÃ©trer l'application afin que les notebooks</u>, <br />\n",
    "   comme le reste de nos donnÃ©es de travail, <u>soient enregistrÃ©s directement sur S3</u>.\n",
    "![CrÃ©er un cluster](img/EMR_configuration_logiciels.png)\n",
    "\n",
    "#### 4.6.1.2 Modifier les paramÃ¨tres du logiciel\n",
    "\n",
    "<u>ParamÃ©trez la persistance des notebooks crÃ©Ã©s et ouvert via JupyterHub</u> :\n",
    "- On peut Ã  cette Ã©tape effectuer des demandes de paramÃ©trage particuliÃ¨res sur nos applications. <br />\n",
    "  L'objectif est, comme pour le reste de nos donnÃ©es de travail, <br />\n",
    "  d'Ã©viter toutes les problÃ©matiques Ã©voquÃ©es prÃ©cÃ©demment. <br />\n",
    "  C'est l'objectif Ã  cette Ã©tape, <u>nous allons enregistrer <br />\n",
    "  et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme <br />\n",
    "  ce serait le cas dans la configuration par dÃ©faut de JupyterHub) mais <br />\n",
    "  <u>directement sur **Amazon S3**</u>.\n",
    "- <u>deux solutions sont possibles pour rÃ©aliser cela</u> :\n",
    " 1. CrÃ©er un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin dâaccÃ¨s au fichier JSON\n",
    " 2. Rentrez directement la configuration au format JSON\n",
    " \n",
    "J'ai personnellement crÃ©Ã© un fichier JSON lors de la crÃ©ation de ma premiÃ¨re instance EMR, <br />\n",
    "puis lorsqu'on dÃ©cide de cloner notre serveur pour en recrÃ©er un facilement Ã  l'identique, <br />\n",
    "la configuration du fichier JSON se retrouve directement copiÃ© comme dans la capture ci-dessous.\n",
    "\n",
    "<u>Voici le contenu de mon fichier JSON</u> :  [{\"classification\":\"jupyter-s3-conf\",\"properties\":{\"s3.persistence.bucket\":\"p8-data\",\"s3.persistence.enabled\":\"true\"}}]\n",
    " Appuyez ensuite sur \"**Suivant**\"\n",
    "![Modifier les paramÃ¨tres du logiciel](img/EMR_parametres_logiciel.png)\n",
    "\n",
    "### 4.6.2 Ãtape 2 : MatÃ©riel\n",
    "\n",
    "A cette Ã©tape, laissez les choix par dÃ©faut. <br />\n",
    "<u>L'important ici est la sÃ©lection de nos instances</u> :\n",
    "\n",
    "1. je choisi les instances de type **M5** qui sont des **instances de type Ã©quilibrÃ©s**\n",
    "2. je choisi le type **xlarge** qui est l'instance la **moins onÃ©reuse disponible**\n",
    " [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
    "3. Je sÃ©lectionne **1 instance MaÃ®tre** (le driver) et **2 instances Principales** (les workeurs) <br />\n",
    "   soit **un total de 3 instance EC2**.\n",
    "![Choix du materiel](img/EMR_materiel.png)\n",
    "\n",
    "### 4.6.3 Ãtape 3 : ParamÃ¨tres de cluster gÃ©nÃ©raux\n",
    "\n",
    "#### 4.6.3.1 Options gÃ©nÃ©rales\n",
    "<u>La premiÃ¨re chose Ã  faire est de donner un nom au cluster</u> :<br />\n",
    "*J'ai Ã©galement dÃ©cochÃ© \"Protection de la rÃ©siliation\" pour des raisons pratiques.*\n",
    "    \n",
    "![Nom du Cluster](img/EMR_nom_cluster.png)\n",
    "\n",
    "#### 4.6.3.2 Actions d'amorÃ§age\n",
    "\n",
    "Nous allons Ã  cette Ã©tape **choisir les packages manquants Ã  installer** et qui <br />\n",
    "nous serons utiles dans l'exÃ©cution de notre notebook.<br />\n",
    "<u>L'avantage de rÃ©aliser cette Ã©tape maintenant est que les packages <br />\n",
    "installÃ©s le seront sur l'ensemble des machines du cluster</u>.\n",
    "\n",
    "La procÃ©dure pour crÃ©er le fichier **bootstrap** qui contient <br />\n",
    "l'ensemble des instructions permettant d'installer tous <br />\n",
    "les packages dont nous aurons besoin est expliquÃ© dans <br />\n",
    "le cours [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n",
    "\n",
    "Nous crÃ©ons donc un fichier nommÃ© \"**bootstrap-emr.sh**\" que nous <u>uploadons <br />\n",
    "sur S3</u>(je lâinstalle Ã  la racine de mon **bucket \"p8-data\"**) et nous l'ajoutons <br />\n",
    "comme indiquÃ© dans la capture d'Ã©cran ci-dessous:\n",
    "![Actions d'amorcage](img/EMR_amorcage.png)\n",
    "\n",
    "Voici le contenu du fichier **bootstrap-emr.sh**<br />\n",
    "Comme on peut le constater il s'agit simplement de commande \"**pip install**\" <br />\n",
    "pour **installer les bibliothÃ¨ques manquantes** comme rÃ©alisÃ© en local.<br />\n",
    "Une fois encore, <u>il est nÃ©cessaire de rÃ©aliser ces actions Ã  cette Ã©tape</u> <br />\n",
    "pour que <u>les packages soient installÃ©s sur l'ensemble des machines du cluster</u> <br />\n",
    "et non pas uniquement sur le driver, comme cela serait le cas si nous exÃ©cutions <br />\n",
    "ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connectÃ© au driver).\n",
    "![Contenu du fichier bootstrap](img/EMR_bootstrap.png)\n",
    "\n",
    "**setuptools** et **pip** sont mis Ã  jour pour Ã©viter une problÃ©matique <br />\n",
    "avec l'installation du package **pyarrow**.<br />\n",
    "**Pandas** a eu droit Ã  une mise Ã  jour majeur (1.3.0) il y a moins d'une semaine <br />\n",
    "au moment de la rÃ©daction de ce notebook, et la nouvelle version de **Pandas** <br />\n",
    "nÃ©cessite une version plus rÃ©cente de **Numpy** que la version installÃ©e par <br />\n",
    "dÃ©faut (1.16.5) Ã  l'initialisation des instances **EC2**. <u>Il ne semble pas <br />\n",
    "possible d'imposer une autre version de Numpy que celle installÃ© par <br />\n",
    "dÃ©faut</u> mÃªme si on force l'installation d'une version rÃ©cente de **Numpy** <br />\n",
    "(en tout cas, ni simplement ni intuitivement).<br />\n",
    "La mise Ã  jour Ã©tant trÃ¨s rÃ©cente <u>la version de **Numpy** n'est pas encore <br />\n",
    "mise Ã  jour sur **EC2**</u> mais on peut imaginer que ce sera le cas trÃ¨s rapidement <br />\n",
    "et il ne sera plus nÃ©cessaire d'imposer une version spÃ©cifique de **Pandas**.<br />\n",
    "En attendant, je demande <u>l'installation de l'avant derniÃ¨re version de **Pandas (1.2.5)**</u>\n",
    "\n",
    "On clique ensuite sur ***Suivant***\n",
    "\n",
    "### 4.6.4 Ãtape 4 : SÃ©curitÃ©\n",
    "\n",
    "#### 4.6.4.1 Options de sÃ©curitÃ©\n",
    "\n",
    "A cette Ã©tape nous sÃ©lectionnons la **paire de clÃ©s EC2** crÃ©Ã© prÃ©cÃ©demment. <br />\n",
    "Elle nous permettra de se connecter en **ssh** Ã  nos **instances EC2** <br />\n",
    "sans avoir Ã  entrer nos login/mot de passe.<br />\n",
    "On laisse les autres paramÃ¨tres par dÃ©faut. <br />\n",
    "Et enfin, on clique sur \"***CrÃ©er un cluster***\"\n",
    " \n",
    "![EMR SÃ©curitÃ©](img/EMR_securite.png)\n",
    "\n",
    "## 4.7 Instanciation du serveur\n",
    "\n",
    "Il ne nous reste plus qu'Ã  attendre que le serveur soit prÃªt. <br />\n",
    "Cette Ã©tape peut prendre entre **15 et 20 minutes**.\n",
    "\n",
    "<u>Plusieurs Ã©tapes s'enchaÃ®ne, on peut suivre l'avancÃ© du statut du **cluster EMR**</u> :\n",
    "\n",
    "![Instanciation Ã©tape 1](img/EMR_instanciation_01.png)\n",
    "![Instanciation Ã©tape 2](img/EMR_instanciation_02.png)\n",
    "![Instanciation Ã©tape 3](img/EMR_instanciation_03.png)\n",
    "\n",
    "<u>Lorsque le statut affiche en vert: \"**En attente**\" cela signifie que l'instanciation <br />\n",
    "s'est bien dÃ©roulÃ©e et que notre serveur est prÃªt Ã  Ãªtre utilisÃ©</u>. \n",
    "\n",
    "## 4.8 CrÃ©ation du tunnel SSH Ã  l'instance EC2 (MaÃ®tre)\n",
    "\n",
    "### 4.8.1 CrÃ©ation des autorisations sur les connexions entrantes\n",
    "\n",
    "<u>Nous souhaitons maintenant pouvoir accÃ©der Ã  nos applications</u> :\n",
    " - **JupyterHub** pour l'exÃ©cution de notre notebook\n",
    " - **Serveur d'historique Spark** pour le suivi de l'exÃ©cution <br />\n",
    "   des tÃ¢ches de notre script lorsqu'il sera lancÃ©\n",
    " \n",
    "Cependant, <u>ces applications ne sont accessibles que depuis le rÃ©seau local du driver</u>, <br />\n",
    "et pour y accÃ©der nous devons **crÃ©er un tunnel SSH vers le driver**.\n",
    "\n",
    "Par dÃ©faut, ce driver se situe derriÃ¨re un firewall qui bloque l'accÃ¨s en SSH. <br />\n",
    "<u>Pour ouvrir le port 22 qui correspond au port sur lequel Ã©coute le serveur SSH, <br />\n",
    "il faut modifier le **groupe de sÃ©curitÃ© EC2 du driver**</u>.\n",
    "\n",
    "Cette Ã©tape est dÃ©crite dans le cours [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / Lancement d'une application Ã  partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512): \n",
    "\n",
    "*Il faudra que l'on se connecte en SSH au driver de notre cluster. <br />\n",
    "Par dÃ©faut, ce driver se situe derriÃ¨re un firewall qui bloque l'accÃ¨s en SSH. <br />\n",
    "Pour ouvrir le port 22 qui correspond au port sur lequel Ã©coute le serveur SSH, <br />\n",
    "il faut modifier le groupe de sÃ©curitÃ© EC2 du driver. Sur la page de la console <br />\n",
    "consacrÃ©e Ã  EC2, dans l'onglet \"RÃ©seau et sÃ©curitÃ©\", cliquez sur \"Groupes de sÃ©curitÃ©\". <br />\n",
    "Vous allez devoir modifier le groupe de sÃ©curitÃ© dâElasticMapReduce-Master. <br />\n",
    "Dans l'onglet \"Entrant\", ajoutez une rÃ¨gle SSH dont la source est \"N'importe oÃ¹\" <br />\n",
    "(ou \"Mon IP\" si vous disposez d'une adresse IP fixe).*\n",
    "\n",
    "![Configuration autorisation ports entrants pour ssh](img/EMR_config_ssh_01.png)\n",
    "\n",
    "<u>Une fois cette Ã©tape rÃ©alisÃ©e vous devriez avoir une configuration semblable Ã  la mienne</u> :\n",
    "\n",
    "![Configuration ssh terminÃ©e](img/EMR_config_ssh_02.png)\n",
    "\n",
    "### 4.8.2 CrÃ©ation du tunnel ssh vers le Driver\n",
    "\n",
    "On peut maintenant Ã©tablir le **tunnel SSH** vers le **Driver**. <br />\n",
    "Pour cela on rÃ©cupÃ¨re les informations de connexion fournis par Amazon <br />\n",
    "depuis la page du service EMR / Cluster / onglet RÃ©capitulatif en <br />\n",
    "cliquant sur \"**Activer la connexion Web**\"\n",
    "\n",
    "![Activer la connexion Web](img/EMR_tunnel_ssh_01.png)\n",
    "\n",
    "<u>On rÃ©cupÃ¨re ensuite la commande fournis par Amazon pour **Ã©tablir le tunnel SSH**</u> :\n",
    "\n",
    "![RÃ©cupÃ©rer la commande pour Ã©tablir le tunnel ssh](img/EMR_tunnel_ssh_02.png)\n",
    "\n",
    "<u>Dans mon cas, la commande ne fonctionne pas tel</u> quel et j'ai du **l'adapter Ã  ma configuration**. <br />\n",
    "La **clÃ© ssh** se situe dans un dossier \"**.ssh**\" elle-mÃªme situÃ©e dans <br />\n",
    "mon **rÃ©pertoire personnel** dont le symbole est, sous Linux, identifiÃ© par un tilde \"**~**\".\n",
    "\n",
    "Ayant suivi le cours [RÃ©alisez des calculs distribuÃ©s sur des donnÃ©es massives / Lancement d'une application Ã  partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives) <br />\n",
    "j'ai choisi d'utiliser le port **5555** au lieu du **8157**, mÃªme si le choix n'est pas trÃ¨s important.<br />\n",
    "    j'ai Ã©galement rencontrÃ© un <u>problÃ¨me de compatibilitÃ©</u> avec <br />\n",
    "l'argument \"**-N**\" (liste des arguments et leur significations <br />\n",
    "disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) j'ai dÃ©cidÃ© de simplement le supprimer.\n",
    "\n",
    "<u>Finalement, j'utilise la commande suivante dans un terminal pour Ã©tablir <br />\n",
    "    mon tunnel ssh (seul l'URL change d'une instance Ã  une autre)</u> : <br />\n",
    "\"**ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com**\"\n",
    "\n",
    "<u>On inscrit \"**yes**\" pour valider la connexion et si <br />\n",
    "    la connexion est Ã©tablit on obtient le rÃ©sultat suivant</u> :\n",
    "\n",
    "![CrÃ©ation du tunnel SSH](img/EMR_connexion_ssh_01.png)\n",
    "\n",
    "Nous avons **correctement Ã©tabli le tunnel ssh avec le driver** sur le port \"5555\".\n",
    "\n",
    "### 4.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une derniÃ¨re Ã©tape est nÃ©cessaire pour accÃ©der Ã  nos applications, <br />\n",
    "en demandant Ã  notre navigateur d'emprunter le tunnel ssh.<br />\n",
    "J'utilise pour cela **FoxyProxy**.\n",
    "[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n",
    "\n",
    "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut Ã  gauche <br />\n",
    "puis renseigner les Ã©lÃ©ments comme dans la capture ci-dessous :\n",
    "\n",
    "![Configuration FoxyProxy Etape 1](img/EMR_foxyproxy_config_01.png)\n",
    "\n",
    "<u>On obtient le rÃ©sultat ci-dessous</u> :\n",
    "\n",
    "![Configuration FoxyProxy Etape 2](img/EMR_foxyproxy_config_02.png)\n",
    "\n",
    "\n",
    "### 4.8.4 AccÃ¨s aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "\n",
    "<u>Avant d'Ã©tablir notre **tunnel ssh** nous avions Ã§a</u> :\n",
    "\n",
    "![avant tunnel ssh](img/EMR_tunnel_ssh_avant.png)\n",
    "\n",
    "<u>On active le **tunnel ssh** comme vu prÃ©cÃ©demment puis on demande <br />\n",
    "Ã  notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
    "\n",
    "![FoxyProxy activation](img/EMR_foxyproxy_activation.png)\n",
    "\n",
    "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
    "\n",
    "![avant tunnel ssh](img/EMR_tunnel_ssh_apres.png)\n",
    "\n",
    "## 4.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter Ã  **JupyterHub** en vue d'exÃ©cuter notre **notebook**, <br />\n",
    "il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparu <br />\n",
    "depuis que nous avons configurÃ© le **tunnel ssh** et **foxyproxy** sur <br />\n",
    "notre navigateur (actualisez la page si ce nâest pas le cas).\n",
    "\n",
    "![DÃ©marrage de JupyterHub](img/EMR_jupyterhub_connexion_01.png)\n",
    "\n",
    "On passe les Ã©ventuels avertissements de sÃ©curitÃ© puis <br />\n",
    "nous arrivons sur une page de connexion.\n",
    "    \n",
    "<u>On se connecte avec les informations par dÃ©faut</u> :\n",
    " - <u>login</u>: **jovyan**\n",
    " - <u>password</u>: **jupyter**\n",
    " \n",
    "![Connexion Ã  JupyterHub](img/EMR_jupyterhub_connexion_02.png)\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook.<br />\n",
    "Il suffit d'en crÃ©er un en cliquant sur \"**New**\" en haut Ã  droite.\n",
    "\n",
    "![Liste et crÃ©ation des notebook](img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Il est Ã©galement possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
    "\n",
    "Grace Ã  la <u>**persistance** paramÃ©trÃ©e Ã  l'instanciation du cluster <br />\n",
    "nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
    "\n",
    "![Notebook stockÃ©s sur S3](img/EMR_jupyterhub_S3.png)\n",
    "\n",
    "Je dÃ©cide d'**importer un notebook dÃ©jÃ  rÃ©digÃ© en local directement <br />\n",
    "sur S3** et je l'ouvre depuis **l'interface JupyterHub**.\n",
    "\n",
    "## 4.10 ExÃ©cution du code\n",
    "\n",
    "Je dÃ©cide d'exÃ©cuter cette partie du code depuis **JupyterHub hÃ©bergÃ© sur notre cluster EMR**.<br />\n",
    "Pour ne pas alourdir inutilement les explications du **notebook**, je ne rÃ©expliquerai pas les Ã©tapes communes <br />\n",
    "que nous avons dÃ©jÃ  vues dans la premiÃ¨re partie oÃ¹ l'on a exÃ©cutÃ© le code localement sur notre machine virtuelle Ubuntu.\n",
    "\n",
    "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
    "\n",
    "**En utilisant ce kernel, une session spark est crÃ©Ã© Ã  l'exÃ©cution de la premiÃ¨re cellule**. <br />\n",
    "Il n'est donc **plus nÃ©cessaire d'exÃ©cuter le code \"spark = (SparkSession ...\"** comme lors <br />\n",
    "de l'exÃ©cution de notre notebook en local sur notre VM Ubuntu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759c1e",
   "metadata": {},
   "source": [
    "### 4.10.1 DÃ©marrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'exÃ©cution de cette cellule dÃ©marre l'application Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba202f",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb788991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac9832",
   "metadata": {},
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages nÃ©cessaires ont Ã©tÃ© installÃ© via l'Ã©tape de **bootstrap** Ã  l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad562eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83663cbd",
   "metadata": {},
   "source": [
    "### 4.10.4 DÃ©finition des PATH pour charger les images et enregistrer les rÃ©sultats\n",
    "\n",
    "Nous accÃ©dons directement Ã  nos **donnÃ©es sur S3** comme si elles Ã©taient **stockÃ©es localement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 's3://p8-data'\n",
    "PATH_Data = PATH+'/Test'\n",
    "PATH_Result = PATH+'/Results'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf883c20",
   "metadata": {},
   "source": [
    "### 4.10.5 Traitement des donnÃ©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe93f5",
   "metadata": {},
   "source": [
    "#### 4.10.5.1 Chargement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32ac34",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ab808",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15b199",
   "metadata": {},
   "source": [
    "#### 4.10.5.2 PrÃ©paration du modÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d497f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fe2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032f135",
   "metadata": {},
   "source": [
    "#### 4.10.5.3 DÃ©finition du processus de chargement des images <br/> et application de leur featurisation Ã  travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933100cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23206e8",
   "metadata": {},
   "source": [
    "#### 4.10.5.4 ExÃ©cutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d760c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a930b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe01b72",
   "metadata": {},
   "source": [
    "### 4.10.6 Chargement des donnÃ©es enregistrÃ©es et validation du rÃ©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29205ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72974aab",
   "metadata": {},
   "source": [
    "<u>On peut Ã©galement constater la prÃ©sence des fichiers <br />\n",
    "    au format \"**parquet**\" sur le **serveur S3**</u> :\n",
    "\n",
    "![Affichage des rÃ©sultats sur S3](img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des tÃ¢ches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des tÃ¢ches en cours <br />\n",
    "avec le **serveur d'historique Spark**.\n",
    "\n",
    "![AccÃ¨s au serveur d'historique spark](img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est Ã©galement possible de revenir et d'Ã©tudier les tÃ¢ches <br />\n",
    "qui ont Ã©tÃ© rÃ©alisÃ©, afin de debugger, optimiser les futurs <br />\n",
    "tÃ¢ches Ã  rÃ©aliser.**\n",
    "\n",
    "<u>Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result)**\" <br />\n",
    "Ã©tait en cours, nous pouvions observer son Ã©tat d'avancement</u> :\n",
    "\n",
    "![Progression execution script](img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "<u>Le **serveur d'historique Spark** nous permet une vision beaucoup plus prÃ©cise <br />\n",
    "de l'exÃ©cution des diffÃ©rentes tÃ¢che sur les diffÃ©rentes machines du cluster</u> :\n",
    "\n",
    "![Suivi des tÃ¢ches spark](img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut Ã©galement constater que notre cluster de calcul a mis <br />\n",
    "un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](img/EMR_SHSpark_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d65bf",
   "metadata": {},
   "source": [
    "## 4.12 RÃ©siliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant terminÃ©. <br />\n",
    "Le cluster de machines EMR est **facturÃ© Ã  la demande**, <br />\n",
    "et nous continuons d'Ãªtre facturÃ© mÃªme lorsque <br />\n",
    "les machines sont au repos.<br />\n",
    "Pour **optimiser la facturation**, il nous faut <br />\n",
    "maintenant **rÃ©silier le cluster**.\n",
    "\n",
    "<u>Je rÃ©alise cette commande depuis l'interface AWS</u> :\n",
    "\n",
    "1. Commencez par **dÃ©sactiver le tunnel ssh dans FoxyProxy** pour Ã©viter des problÃ¨mes de **timeout**.\n",
    "![DÃ©sactivation de FoxyProxy](img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**RÃ©silier**\"\n",
    "![Cliquez sur RÃ©silier](img/EMR_resiliation_01.png)\n",
    "3. Confirmez la rÃ©siliation\n",
    "![Confirmez la rÃ©siliation](img/EMR_resiliation_02.png)\n",
    "4. La rÃ©siliation prend environ **1 minute**\n",
    "![RÃ©siliation en cours](img/EMR_resiliation_03.png)\n",
    "5. La rÃ©siliation est effectuÃ©e\n",
    "![RÃ©siliation terminÃ©e](img/EMR_resiliation_04.png)\n",
    "\n",
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau exÃ©cuter notre notebook dans les mÃªmes conditions, <br />\n",
    "il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle <br />\n",
    "sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    " 1. Cliquez sur \"**Cloner**\"\n",
    "   ![Cloner un cluster](img/EMR_cloner_01.png)\n",
    " 2. Dans notre cas nous ne souhaitons pas inclure d'Ã©tapes\n",
    "   ![Ne pas inclure d'Ã©tapes](img/EMR_cloner_02.png)\n",
    " 3. La configuration du cluster est recrÃ©Ã©e Ã  lâidentique. <br />\n",
    "    On peut revenir sur les diffÃ©rentes Ã©tapes si on souhaite apporter des modifications<br />\n",
    "    Quand tout est prÃªt, cliquez sur \"**CrÃ©er un cluster**\"\n",
    "  ![VÃ©rification/Modification/CrÃ©er un cluster](img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'installÃ© et de configurÃ© et en s'assurant <br />\n",
    "   de s'attribuer les droits nÃ©cessaires sur le compte AMI utilisÃ©)\n",
    " 1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    " ![Exporter AWS CLI](img/EMR_cloner_cli_01.png)\n",
    " 2. Copier/Coller la commande **depuis un terminal**\n",
    " ![Copier Coller Commande](img/EMR_cloner_cli_02.png)\n",
    "\n",
    "## 4.14 Arborescence du serveur S3 Ã  la fin du projet\n",
    "\n",
    "<u>Pour information, voici **l'arborescence complÃ¨te de mon bucket S3 p8-data** Ã  la fin du projet</u> : <br />\n",
    "*Par soucis de lisibilitÃ©, je ne liste pas les 131 sous dossiers du rÃ©pertoire \"Test\"*\n",
    "\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P8_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba46f9",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Nous avons rÃ©alisÃ© ce projet **en deux temps** en tenant <br />\n",
    "compte des contraintes qui nous ont Ã©tÃ© imposÃ©es.\n",
    "\n",
    "Nous avons **dans un premier temps dÃ©veloppÃ© notre solution en local** <br />\n",
    "sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>premiÃ¨re phase</u> a consistÃ© Ã  **installer l'environnement de travail Spark**. <br />\n",
    "**Spark** a un paramÃ¨tre qui nous permet de travaillÃ© en local et nous permet <br />\n",
    "ainsi de **simuler du calcul partagÃ©** en considÃ©rant <br />\n",
    "**chaque cÅur d'un processeur comme un worker indÃ©pendant**.<br />\n",
    "Nous avons travaillÃ© sur un plus **petit jeu de donnÃ©e**, l'idÃ©e Ã©tait <br />\n",
    "simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de rÃ©aliser du **transfert learning** <br />\n",
    "Ã  partir du model **MobileNetV2**.<br />\n",
    "Ce modÃ¨le a Ã©tÃ© retenu pour sa **lÃ©gÃ¨retÃ©** et sa **rapiditÃ© d'exÃ©cution** <br />\n",
    "ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les rÃ©sultats ont Ã©tÃ© enregistrÃ©s sur disque en plusieurs <br />\n",
    "partitions au format \"**parquet**\".\n",
    "\n",
    "<u>**La solution a parfaitement fonctionnÃ© en mode local**</u>.\n",
    "\n",
    "La <u>deuxiÃ¨me phase</u> a consistÃ© Ã  crÃ©er un **rÃ©el cluster de calculs**. <br />\n",
    "L'objectif Ã©tait de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a Ã©tÃ© l'utilisation du prestataire de services **Amazon Web Services** <br />\n",
    "qui nous permet de **louer Ã  la demande de la puissance de calculs**, <br />\n",
    "pour un **coÃ»t tout Ã  fait acceptable**.<br />\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure As A Service** (IAAS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus <br />\n",
    "haut niveau (**Plateforme As A Service** PAAS)<br />\n",
    "en utilisant le service **EMR** qui nous permet d'un seul coup <br />\n",
    "d'**instancier plusieurs serveur (un cluster)** sur lesquels <br />\n",
    "nous avons pu demander l'installation et la configuration de plusieurs<br />\n",
    "programmes et librairies nÃ©cessaires Ã  notre projet comme **Spark**, <br />\n",
    "**Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n",
    "\n",
    "En plus d'Ãªtre plus **rapide et efficace Ã  mettre en place**, nous avons <br />\n",
    "la **certitude du bon fonctionnement de la solution**, celle-ci ayant Ã©tÃ© <br />\n",
    "prÃ©alablement validÃ© par les ingÃ©nieurs d'Amazon.\n",
    "\n",
    "Nous avons Ã©galement pu installer, sans difficultÃ©, **les packages <br />\n",
    "nÃ©cessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec trÃ¨s peu de modification, et plus simplement encore, <br />\n",
    "nous avons pu **exÃ©cuter notre notebook comme nous l'avions fait localement**.<br />\n",
    "Nous avons cette fois-ci exÃ©cutÃ© le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons optÃ© pour le service **Amazon S3** pour **stocker les donnÃ©es de notre projet**. <br />\n",
    "S3 offre, pour un faible coÃ»t, toutes les conditions dont nous avons besoin pour stocker <br />\n",
    "et exploiter de maniÃ¨re efficace nos donnÃ©es.<br />\n",
    "L'espace allouÃ© est potentiellement **illimitÃ©**, mais les coÃ»ts seront fonction de l'espace utilisÃ©.\n",
    "\n",
    "Il nous sera **facile de faire face Ã  une montÃ© de la charge de travail** en **redimensionnant** <br />\n",
    "simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), <br />\n",
    "les coÃ»ts augmenteront en consÃ©quence mais resteront nettement infÃ©rieurs aux coÃ»ts engendrÃ©s <br />\n",
    "par l'achat de matÃ©riels ou par la location de serveurs dÃ©diÃ©s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
